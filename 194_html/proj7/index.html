<!DOCTYPE html>
<!-- saved from url=(0078)https://inst.eecs.berkeley.edu/~cs194-26/fa16/upload/files/proj3/cs194-26-ach/ -->
<html>

<head>
  <title>Jeffrey Zhang</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=windows-1252" />
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Tangerine&amp;v1" />
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz" />
  <link rel="stylesheet" type="text/css" href="../../style/style.css" />
</head>

<body>
  <div id="main">
    <div id="header">
      <div id="logo">
        <h1>Jeffrey Zhang<a href="#"></a></h1>
      </div>
      <div id="menubar">
        <ul id="menu">
          <!-- put class="current" in the li tag for the selected page - to highlight which page you're on -->
          <li><a href="../../index.html">Home</a></li>
          <!-- <li><a href="examples.html">Examples</a></li> -->
          <li><a href="../../research.html">Research</a></li>
          <li class="current"><a href="../../projects.html">Projects</a></li>
          <li><a href="../../hobbies.html">Hobbites/Interests</a></li>
        </ul>
      </div>
    </div>
    <div id="site_content"><span>&#8592;</span>
        <a href='../../194.html'>Back to 194 Projects</a>
      <div id="content">
<html class="gr__inst_eecs_berkeley_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><style>
    p    {font-size: 125%;}
    h1    {font-size: 200%;}
    h2    {font-size: 175%;}
    h3    {font-size: 150%;}
</style>
  
    </head><body style="padding-left: 5%; width: 90%; background-color: #EEE;" data-gr-c-s-loaded="true" class="vsc-initialized"><center>
        <h1>Project 7: Image Morphing and Mosaicing</h1>
        <h3>Jeffrey Zhang</h3>
        <h4>CS194-26 | Professor A. Efros</h4>
    </center>



    <section>
        <h2>Overview</h2>
        <hr>
        <p>
          In this project, I take images and alter the perspective in an the image by calculating a homography matrix to transform the images to the desired perspective.  I can apply this concept to stitch multiple images of the same scene from different perspectives by correcting the persepctives of each image and aligning them together to make a panorama of the images!
        </p>
    </section>
    <section>
        <h2>Calculating Homographies</h2>
        <hr>
        <p>
          The homography matrix is a 3x3 matrix that transforms a point in the original image to a desired point to achieve a certain perspective. This will follow the following equation, where x and y are our original image points and x' and y' are points in our transformed image:
        </p>
        <center><img src="./index_files/matrix1.png" style="height: 200px;"></center>
        <p>
          Because we add an extra value 1 to all our points to make a 3x1 matrix, we know i must equal to 1. Thus, we have 8 variables to solve, which requires a minimum of 4 unique points to generate 8 equations. Redefining some variables and making sure w = 1, we can define each point by the following equation.
        </p>
        <center><img src="./index_files/equations.png" style="height: 300px;"></center>  
        <p>
          Thus, combining all the equations for each point into matrix notation, we can solve for the variables in our homography matrix!
        </p>
        <center><img src="./index_files/matrix_elements.png" style="height: 300px;"></center>  
        <center>A few of the images courtesy of Penn State's CSE 486 class slides</center>

    </section>

     <section>
        <h2>Warping Images</h2>
        <hr>
        <p>
          After calculating the homography matrix, I can now warp my image into a different perspective. I found all the coordinate points in my original image and apply the homography matrix to each point to find it's corresponding coordinate in my new image. This can be done using numpy arrays to vectorize the process (no for loops), which runs almost instantly.
        </p>
    </section>

    <section>
        <h2>Image Rectification</h2>
        <hr>
        <p>
          Here are several images to demonstrate image rectification. Below is an image of a concept smart building rectified from two different perspectives and a picture of Berkeley rectified from one other perspective.
        </p>
         <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Original image</th>
                <th>Rectified image</th>
            </tr>
            <tr>
                <th><img src="./index_files/orig1.png"  style="width: 100%;"></th>
                <th><img src="./index_files/rec1.png"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Original image</th>
                <th>Rectified image</th>
            </tr>
            <tr>
                <th><img src="./index_files/orig2.png"  style="width: 100%;"></th>
                <th><img src="./index_files/rec2.png"  style="width: 100%;""></th>
            </tr>
        </tbody></table>
        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Original image</th>
                <th>Rectified image</th>
            </tr>
            <tr>
                <th><img src="./index_files/orig3.png"  style="width: 100%;"></th>
                <th><img src="./index_files/rec3.png"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <p>
            Notice that the smart building's perspective warping was much cleaner than the picture of Berkeley. This is because the smart building picture is an artist's conception and is computer drawn so there is no distortion in the image. However, in the perspective warping of Berkeley, there is distortion occuring near the edges of the image. This is likely due to lens distortion around the edges of the original picture, which alters the integrity of the perspective.
        </p>
    </section>

    <section>
        <h2>Blending Images into a Mosaic</h2>
        <hr>
        <p>
          Below are images I blended into a mosaic. I rectify images based on correspondence points in the center image and warp the images to the left and right of the image to those correspondence points. I blend 2 images at a time, blending the left and center image first, then blending that result with the right image. 
        </p>
        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th><img src="./index_files/left.png"  style="width: 100%;"></th>
                <th><img src="./index_files/right.png"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <center><img src="./index_files/mosaic2.png"  style="width: 100%;"></center>
        <p>
          To create a seamless blend, I implement an alpha blending technique by finding the overlap between the two images and defining the pixels in the overlap as im1_pixel*alpha + im2_pixel*(1-alpha), where alpha is in the range 0 to 1 depending on the column of the pixel. This defines the center column of the overlap to have alpha = 0.5 and the left and right edges of the overlap to be either 0 or 1 depending on the side. You can see a difference below:
        </p>
        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Without alpha blending</th>
                <th>With alpha blending</th>
            </tr>
            <tr>
                <th><img src="./index_files/noalpha.png"  style="width: 100%;"></th>
                <th><img src="./index_files/alpha.png"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <h3>Mosaic Results</h3>
        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Image 1</th>
                <th>Image 2</th>
                <th>Image 3</th>
            </tr>
            <tr>
                <th><img src="./index_files/im1_1.jpg"  style="width: 100%;"></th>
                <th><img src="./index_files/im1_2.jpg"  style="width: 100%;"></th>
                <th><img src="./index_files/im1_3.jpg"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <center><h4>Combined Mosaic</h4></center>
        <center><img src="./index_files/mosaic1.png"  style="width: 100%;"></center>
        <center><p>Notice the center is a bit blurry because my center of projection was not perfectly aligned between images.</p></center>

        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Image 1</th>
                <th>Image 2</th>
                <th>Image 3</th>
            </tr>
            <tr>
                <th><img src="./index_files/im2_1.jpg"  style="width: 100%;"></th>
                <th><img src="./index_files/im2_2.jpg"  style="width: 100%;"></th>
                <th><img src="./index_files/im2_3.jpg"  style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <center>Combined Mosaic</center>
        <center><img src="./index_files/mosaic2.png"  style="width: 100%;"></center>

        <table align="center" style="width: 95%;">
            <tbody><tr>
                <th>Image 1</th>
                <th>Image 2</th>
                <th>Image 3</th>
            </tr>
            <tr>
                <th><img src="./index_files/im3_1.jpg"  style="width: 100%;"></th>
                <th><img src="./index_files/im3_2.jpg" style="width: 100%;"></th>
                <th><img src="./index_files/im3_3.jpg" style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <center>Combined Mosaic</center>
        <center><img src="./index_files/mosaic3.png" style="width: 100%;"></center>

    </section>

    <section>
        <h1>Feature	Matching for Autostitching</h1>
        <hr>
        <p>
          In this second part, we use find feature points in our images to calculate the homography without the need for manual point selection between images. We will follow sections 2-5 from this <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa16/Papers/MOPS.pdf">paper</a> by Brown et al. The paper details finding Harris corners, creating features from the corners, matching features, and finally calculating homography matrices for stitching. Finding all the feature points to create the homography transformation was done on grayscale images. This reduces the number dimensions to work with, but the methods tested below could easily be translated to each color channel R,G, or B. Once points and matrices are calculated, I stitch the color images together!
        </p>
    </section>

    <section>
        <h2>Detecting Corner Features</h2>
        <hr>
        <h3>Harris Corners</h3>
        <p>
          We will use Harris corner features as our first step to extracting important features from the images. Below are all the prominent corners detected with sigma = 2, but as you can tell, we need to filter out the weaker feature points for more a more efficient and effective feature matching algorithm. With so many points (around 20000-30000), our computation time will very high and the potential for redudant and less prominent features are more likley as well.
        </p>
         <table align="center" style="width: 100%;">
            <tbody><tr>
                <th>Image 1</th>
                <th>Image 2</th>
            </tr>
            <tr>
                <th><img src="./index_files/image1.png" style="width: 100%;"></th>
                <th><img src="./index_files/image2.png" style="width: 100%;"></th>
            </tr>
            <tr>
                <th>Image 1 Harris Corners</th>
                <th>Image 2 Harris Corners</th>
            </tr>
            <tr>
                <th><img src="./index_files/harris1.png" style="width: 100%;"></th>
                <th><img src="./index_files/harris2.png" style="width: 100%;"></th>
            </tr>
        </tbody></table>

        <h3>Adaptive Non-Maximal Suppression</h3>
        <p>
          To filter out the harris corner points, we will use Adaptive Non-Maximal Suppression. ANMS is calculated by finding the largest raidus such that a corner feature is the most prominent (largest eigenvalues) among the points within that raidus. We take the top 500 points with the largest radius, which leaves us with points that most resembles a corner (feature is more distinct) and is (relatively) uniformly distributed. Below are the top 500 anms points for my example images, giving us a workable number of prominent features to work with to start feature matching.
        </p>
        <table align="center" style="width: 100%;">
            <tbody><tr>
                <th>Image 1 Top 500 ANMS</th>
                <th>Image 2 Top 500 ANMS</th>
            </tr>
            <tr>
                <th><img src="./index_files/anms1.png" style="width: 100%;"></th>
                <th><img src="./index_files/anms2.png" style="width: 100%;"></th>
            </tr>
        </tbody></table>
    </section>
 
   <section>
        <h2>Feature Descriptor Extraction/ Matching</h2>
        <hr>
        <p>
          For each of the feature point, we want a way to represent the value of that point. One way we can do this is by taking a 40x40 block around the point and downsizing it to an 8x8 image (by Gaussian blurring and sample every 4th pixel). We can then convert this 8x8 matrix into a 1x64 sized vector to represent each feature point.
        </p>
        <p>
          We can represent each feature point as a 1x64 sized vector by taking a 40x40 block around the point and downsizing it to an 8x8 image (by Gaussian blurring and sampling every 4th pixel). We can then convert this 8x8 matrix into a 1x64 sized vector to represent each feature point. Below is an image of features points and it's corresponding 8x8 feature matrices taken from Professor Efros's CS194-26 class slides:
        </p>
        <center>
        	<img src="./index_files/features.png" style="width: 75%;">
        </center>
        <p>
          To match the features, we use the sum of squared difference as our distance metric to match every feature point in image 1 to every feature point in image 2. A naive method would be to take the top percentage of points with the lowest distance metric. However, this creates the potential for incorrectly matching features that are similar but not the same feature point. To prevent this issue, we can use David Lowe's nearest neighbor ratio test, which is 1NN/2NN. The lower the value, the more likely the first nearest neighbor is an accurate/unique matching feature. This reduces the chance of matching different features that are similar in the image. Below are results for images of my apartment room (18 matched features). 
        </p>
        <table align="center" style="width: 100%;">
            <tbody><tr>
                <th>Image 1 Matched Features</th>
                <th>Image 2 Matched Features</th>
            </tr>
            <tr>
                <th><img src="./index_files/matching1.png" style="width: 100%;"></th>
                <th><img src="./index_files/matching2.png" style="width: 100%;"></th>
            </tr>
        </tbody></table>
    </section>

    <section>
        <h2>RANSAC</h2>
        <hr>
        <p>
          We use Random Sample Consensus (RANSAC) to determine the best transformation that includes the most number of matched features (inliers) from the previous step. Inliers are determined by taking the sum of squared differences between features and checking if they are less than some epsilon (e = 10 in my code). The pseudocode for RANSAC is below (taken from Professor Efros's CS194-26 slides):
      	</p>
      	<center>
      	<img src="./index_files/ransac.png" style="width: 75%;">
      	</center>
      	<p>
      	  Running RANSAC on the matched features above, I am able to find 4 points below that sets all 18 matched points as inliers!
      	</p>
        <table align="center" style="width: 100%;">
            <tbody><tr>
                <th>RANSAC on Image 1</th>
                <th>RANSAC on Image 2</th>
            </tr>
            <tr>
                <th><img src="./index_files/ransac1.png" style="width: 100%;"></th>
                <th><img src="./index_files/ransac2.png" style="width: 100%;"></th>
            </tr>
        </tbody></table>
        <p>
          With accurate points for creating a homography matrix, I can now use the warping functions from the previous part to get the correct perspective warp by taking the least squares of all the inlier points:
        </p>
        <center>
          <img src="./index_files/final.png" style="width: 60%;">
        </center>
        <p>
          Here is the stitched image in color:
        </p>
        <center>
          <img src="./index_files/stitch.png" style="width: 75%;">
        </center>
    </section>

    <section>
    <h2>Auto Stitching Results (with comparisons to part 1)</h2>
    <hr>
    	<center>
    	<h3> Soda </h3>
		<h4>Manual Alignment Mosaic</h4>
        <img src="./index_files/mosaic1.png" style="height: 400px;">
        <p>Notice the center is a bit blurry because my center of projection was not perfectly aligned between images and my chosen correspondence points were not perfect.</p>

        <h4>Auto Alignment Mosaic</h4>
        <img src="./index_files/mosaic1(1).png" style="height: 400px;">
       	<p>The automatically detected features chooses the best points to minimize error. As you can see in the center of the image, we no longer have as much ghosting as we did in the previous manually aligned image!</p>
       	</center>

       	<center>
    	<h3> House </h3>
		<h4>Manual Alignment Mosaic</h4>
        <img src="./index_files/mosaic2.png"  style="width: 100%;">
        <p>The manual alignment here seems pretty good</p>

        <h4>Auto Alignment Mosaic</h4>
        <img src="./index_files/mosaic2(1).png" style="width: 100%;">
       	<p>The automatically detected features looks just as good as the manually assigned features</p>
       	</center>

       	<center>
    	<h3> Apartment Room </h3>
		<h4>Manual Alignment Mosaic</h4>
        <img src="./index_files/mosaic3.png" style="width: 100%;">
        <p>You can notice some error in the alignment near the top of the images.</p>

        <h4>Auto Alignment Mosaic</h4>
        <img src="./index_files/mosaic3(1).png"  style="width: 100%;">
       	<p>Notice that the top of the images are almost perfectly (if not perfectly) aligned! The automatic stitching clearly wins here.</p>
       	</center>
    </section>

     <section>
        <h2>Bells and Whistles</h2>
        <hr>
        <h3>Panorama Recognition</h3>
        <p>
          Given a set of images, panorama recognition is used to recognize which images can be stitched together to make a panorama image. I realized this technique is implemented by google photos as google automatically gave me panorama images for the images I took for this project. My implementation of panorama recognition uses pairwise comparisons between photos to determine the best pair of images to be stitched together. The best metric for determining if two images are of the same scene is simply the number of matching features. 
        </p>
        <p>
          I originally set out combining the number of matching features and the number of inliers given the best homography matrix as two metrics for image similarity. However, I found calculating the best homography matrix between an image and all other images was quite a bit of extra computation. After some testing, simply using the number of matching features was enough to give me very accurate results (granted my image set was quite small). After finding which groups each image belonged in, the second step is determining the ordering of images. I did this by comparing the average of the feature points in the x-axis and images with the largest x-axis average would be the most left in the ordering (and vice versa).  Below are the automatically recognized panoramas after feeding in a list of images in the listed order (from left to right)!
        </p>
         <table align="center" style="width: 95%;">
            <tbody><tr>
                <th><img src="./index_files/im1_2.jpg" style="width: 250px;"></th>
                <th><img src="./index_files/im3_1.jpg" style="width:250px;"></th>
                <th><img src="./index_files/im1_1.jpg" style="width: 250px;"></th>
            </tr>
            <tr>
                <th><img src="./index_files/im2_2.jpg" style="width: 250px;"></th>
                <th><img src="./index_files/im3_2.jpg" style="width:250px;"></th>
                <th><img src="./index_files/im2_1.jpg" style="width: 250px;"></th>
            </tr>
        </tbody></table>
        <center>
        <img src="./index_files/bells1.png" style="width: 60%;">
        <img src="./index_files/bells2.png" style="width: 60%;">
        <img src="./index_files/bells3.png" style="width: 60%;">
        </center>
    </section>

    <section>
        <h2>Things Learned</h2>
        <hr>
        <p>
          From part 1: The big assumption we are making with blending images together into a mosaic is that the center of projection is the same for all the images. With my phone, which doesn't have square edges, it was particularly hard to rotate the camera and maintain the same center of projection (moving the aperature location or tilting my phone up/down will shift the center of projection). This results in one set of images not turning out great, but the other images were properly taken.
        </p>
        <p>
          The coolest thing I learned to do in this project is how to implement an automatic feature detection/ matching algorithm. We can see that most of the mosaics created automatically were better aligned than the manually selected feature points.
        </p>
    </section>



<span id="sbmarwbthv5"></span></body></html>